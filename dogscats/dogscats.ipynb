{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jared/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jared/courses/deeplearning2/Kaggle/dogscats/data\n"
     ]
    }
   ],
   "source": [
    "%cd data\n",
    "\n",
    "data_dir = %pwd\n",
    "train_dir = data_dir + '/train/'\n",
    "valid_dir = data_dir + '/valid/'\n",
    "test_dir = data_dir + '/test/'\n",
    "sample_dir = data_dir + '/sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "\n",
    "num_valid = 1000\n",
    "num_sample = 64\n",
    "\n",
    "classes = ['cat', 'dog']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in classes:\n",
    "    os.mkdir(train_dir + cls, 775)\n",
    "    os.mkdir(valid_dir + cls, 775)\n",
    "    os.mkdir(sample_dir + 'train/' + cls, 775)\n",
    "    os.mkdir(sample_dir + 'valid/' + cls, 775)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for cls in classes:\n",
    "    files = glob(train_dir + cls + '*.jpg')\n",
    "    np.random.shuffle(files)\n",
    "\n",
    "    for file in files[:num_sample]:\n",
    "        copyfile(file, sample_dir + 'train/' + cls + '/' + os.path.basename(file))\n",
    "        \n",
    "    for file in files[num_sample:num_sample+num_sample]:\n",
    "        copyfile(file, sample_dir + 'valid/' + cls + '/' + os.path.basename(file))\n",
    "    \n",
    "    for file in files[:num_valid]:\n",
    "        os.rename(file, valid_dir + cls + '/' + os.path.basename(file))\n",
    "        \n",
    "    for file in files[num_valid:]:\n",
    "        os.rename(file, train_dir + cls + '/' + os.path.basename(file))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Flatten, Convolution2D, MaxPooling2D, Input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#base_dir = sample_dir\n",
    "path = data_dir + '/'\n",
    "batch_size = 64\n",
    "image_size = (224,224)\n",
    "image_shape = (*image_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "gen = ImageDataGenerator()\n",
    "val_gen = ImageDataGenerator()\n",
    "\n",
    "batches = gen.flow_from_directory(path+'train/', target_size=image_size, batch_size=batch_size, shuffle=True)\n",
    "val_batches = val_gen.flow_from_directory(path+'valid/', target_size=image_size, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_steps = batches.n//batch_size\n",
    "val_steps = val_batches.n//batch_size\n",
    "\n",
    "if (val_steps == 0):\n",
    "    val_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_2 (Batch (None, 128, 128, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 126, 126, 64)      1792      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 63, 63, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 57600)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 124)               7142524   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 250       \n",
      "=================================================================\n",
      "Total params: 7,181,506\n",
      "Trainable params: 7,181,500\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(input_shape=image_shape)\n",
    "    , Convolution2D(64, (3,3), activation='relu')\n",
    "    , MaxPooling2D((2,2))\n",
    "    , Convolution2D(64, (3,3), activation='relu')\n",
    "    , MaxPooling2D((2,2))\n",
    "    , Flatten()\n",
    "    , Dense(124, activation='relu')\n",
    "    , Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "359/359 [==============================] - 75s - loss: 0.6704 - acc: 0.6494 - val_loss: 0.5757 - val_acc: 0.6699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb020ac59b0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches, train_steps, epochs=1, validation_data=val_batches, validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "                                    class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, target_size=(224,224)):\n",
    "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None, target_size=target_size)\n",
    "    return np.concatenate([batches.next() for i in range((batches.n//batch_size) + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "gen = ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "val_gen = ImageDataGenerator(featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "gen.fit(get_data(path+'train/'))\n",
    "val_gen.fit(get_data(path+'train/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = gen.flow_from_directory(path+'train/', target_size=image_size, batch_size=batch_size, shuffle=False)\n",
    "val_batches = val_gen.flow_from_directory(path+'valid/', target_size=image_size, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "vgg = VGG16(include_top=False, input_shape=image_shape, pooling='avg')\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vgg.predict_generator(batches, train_steps+1)\n",
    "y_train = keras.utils.np_utils.to_categorical(batches.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = vgg.predict_generator(val_batches, val_steps+1)\n",
    "y_valid = val_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000, 512) (2000, 512)\n"
     ]
    }
   ],
   "source": [
    "save_array(path + 'train_vgg_average_pool_features_normalized.bc', X_train)\n",
    "save_array(path + 'valid_vgg_average_pool_features_normalized.bc', X_valid)\n",
    "\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000, 512) (2000, 512)\n"
     ]
    }
   ],
   "source": [
    "X_train = load_array(path + 'train_vgg_average_pool_features_normalized.bc')\n",
    "X_valid = load_array(path + 'valid_vgg_average_pool_features_normalized.bc')\n",
    "\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(batches.classes)\n",
    "y_valid = keras.utils.np_utils.to_categorical(val_batches.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 267,778\n",
      "Trainable params: 265,730\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(input_shape=vgg.output_shape[1:])\n",
    "    , Dense(512, activation='relu')\n",
    "    , BatchNormalization()\n",
    "    , Dropout(.5)\n",
    "    , Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/4\n",
      "23000/23000 [==============================] - 5s - loss: 0.1695 - acc: 0.9393 - val_loss: 0.0902 - val_acc: 0.9655\n",
      "Epoch 2/4\n",
      "23000/23000 [==============================] - 5s - loss: 0.1135 - acc: 0.9561 - val_loss: 0.0817 - val_acc: 0.9670\n",
      "Epoch 3/4\n",
      "23000/23000 [==============================] - 5s - loss: 0.0971 - acc: 0.9602 - val_loss: 0.0878 - val_acc: 0.9675\n",
      "Epoch 4/4\n",
      "23000/23000 [==============================] - 5s - loss: 0.0973 - acc: 0.9621 - val_loss: 0.0789 - val_acc: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ad44e7b70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=4, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0844 - acc: 0.9683 - val_loss: 0.0793 - val_acc: 0.9715\n",
      "Epoch 2/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0796 - acc: 0.9682 - val_loss: 0.0695 - val_acc: 0.9755\n",
      "Epoch 3/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0794 - acc: 0.9687 - val_loss: 0.0806 - val_acc: 0.9760\n",
      "Epoch 4/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0793 - acc: 0.9697 - val_loss: 0.0855 - val_acc: 0.9655\n",
      "Epoch 5/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0638 - acc: 0.9749 - val_loss: 0.0836 - val_acc: 0.9710\n",
      "Epoch 6/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0642 - acc: 0.9755 - val_loss: 0.0748 - val_acc: 0.9740\n",
      "Epoch 7/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0632 - acc: 0.9768 - val_loss: 0.0775 - val_acc: 0.9715\n",
      "Epoch 8/8\n",
      "23000/23000 [==============================] - 5s - loss: 0.0582 - acc: 0.9777 - val_loss: 0.0788 - val_acc: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d1004cb70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = .0001\n",
    "model.fit(X_train, y_train, epochs=8, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0536 - acc: 0.9800 - val_loss: 0.0810 - val_acc: 0.9735\n",
      "Epoch 2/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0493 - acc: 0.9804 - val_loss: 0.0891 - val_acc: 0.9740\n",
      "Epoch 3/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0497 - acc: 0.9820 - val_loss: 0.0889 - val_acc: 0.9695\n",
      "Epoch 4/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0492 - acc: 0.9817 - val_loss: 0.0928 - val_acc: 0.9685\n",
      "Epoch 5/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0485 - acc: 0.9818 - val_loss: 0.0935 - val_acc: 0.9710\n",
      "Epoch 6/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0424 - acc: 0.9838 - val_loss: 0.0971 - val_acc: 0.9725\n",
      "Epoch 7/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0393 - acc: 0.9859 - val_loss: 0.0953 - val_acc: 0.9705\n",
      "Epoch 8/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0382 - acc: 0.9846 - val_loss: 0.0974 - val_acc: 0.9730\n",
      "Epoch 9/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0372 - acc: 0.9865 - val_loss: 0.1060 - val_acc: 0.9710\n",
      "Epoch 10/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0387 - acc: 0.9866 - val_loss: 0.0989 - val_acc: 0.9700\n",
      "Epoch 11/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0405 - acc: 0.9857 - val_loss: 0.0977 - val_acc: 0.9685\n",
      "Epoch 12/12\n",
      "23000/23000 [==============================] - 5s - loss: 0.0361 - acc: 0.9869 - val_loss: 0.1003 - val_acc: 0.9685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d1004cc18>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = .00001\n",
    "model.fit(X_train, y_train, epochs=12, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0329 - acc: 0.9885 - val_loss: 0.1090 - val_acc: 0.9685\n",
      "Epoch 2/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0391 - acc: 0.9862 - val_loss: 0.1042 - val_acc: 0.9710\n",
      "Epoch 3/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0301 - acc: 0.9892 - val_loss: 0.1051 - val_acc: 0.9710\n",
      "Epoch 4/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0318 - acc: 0.9887 - val_loss: 0.1068 - val_acc: 0.9725\n",
      "Epoch 5/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0287 - acc: 0.9901 - val_loss: 0.1088 - val_acc: 0.9745\n",
      "Epoch 6/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0295 - acc: 0.9891 - val_loss: 0.1061 - val_acc: 0.9740\n",
      "Epoch 7/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0282 - acc: 0.9901 - val_loss: 0.1011 - val_acc: 0.9710\n",
      "Epoch 8/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0270 - acc: 0.9901 - val_loss: 0.1012 - val_acc: 0.9705\n",
      "Epoch 9/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0279 - acc: 0.9900 - val_loss: 0.1089 - val_acc: 0.9730\n",
      "Epoch 10/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0274 - acc: 0.9903 - val_loss: 0.1127 - val_acc: 0.9660\n",
      "Epoch 11/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0286 - acc: 0.9906 - val_loss: 0.1083 - val_acc: 0.9685\n",
      "Epoch 12/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0258 - acc: 0.9909 - val_loss: 0.1020 - val_acc: 0.9725\n",
      "Epoch 13/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0267 - acc: 0.9904 - val_loss: 0.1094 - val_acc: 0.9680\n",
      "Epoch 14/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0259 - acc: 0.9920 - val_loss: 0.1136 - val_acc: 0.9675\n",
      "Epoch 15/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0225 - acc: 0.9926 - val_loss: 0.1116 - val_acc: 0.9680\n",
      "Epoch 16/16\n",
      "23000/23000 [==============================] - 5s - loss: 0.0237 - acc: 0.9914 - val_loss: 0.1135 - val_acc: 0.9695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d1004cef0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = .000001\n",
    "model.fit(X_train, y_train, epochs=16, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmented_images(path, image_size, batch_size):\n",
    "    gen = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization=True,\n",
    "        rotation_range=90.,\n",
    "        zoom_range=2.,\n",
    "        fill_mode='nearest',\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "    \n",
    "    gen.fit(get_data(path))\n",
    "\n",
    "    return gen.flow_from_directory(path, target_size=image_size, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "X_train_aug_1 = vgg.predict_generator(get_augmented_images(path+'train/', image_size, batch_size), train_steps+1)\n",
    "X_train_aug_2 = vgg.predict_generator(get_augmented_images(path+'train/', image_size, batch_size), train_steps+1)\n",
    "X_train_aug_3 = vgg.predict_generator(get_augmented_images(path+'train/', image_size, batch_size), train_steps+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path + 'train_vgg_average_pool_features_augmented_normalized_1.bc', X_train_aug_1)\n",
    "save_array(path + 'train_vgg_average_pool_features_augmented_normalized_2.bc', X_train_aug_2)\n",
    "save_array(path + 'train_vgg_average_pool_features_augmented_normalized_3.bc', X_train_aug_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.np_utils.to_categorical(batches.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23000, 512) (23000, 512)\n"
     ]
    }
   ],
   "source": [
    "X_train = load_array(path + 'train_vgg_average_pool_features.bc')\n",
    "X_train_aug_1 = load_array(path + 'train_vgg_average_pool_features_augmented_normalized_1.bc')\n",
    "X_train_aug_2 = load_array(path + 'train_vgg_average_pool_features_augmented_normalized_2.bc')\n",
    "X_train_aug_3 = load_array(path + 'train_vgg_average_pool_features_augmented_normalized_3.bc')\n",
    "\n",
    "print(X_train.shape, X_train_aug_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = load_array(path + 'valid_vgg_average_pool_features_normalized.bc')\n",
    "y_valid = keras.utils.np_utils.to_categorical(val_batches.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 798,210\n",
      "Trainable params: 794,114\n",
      "Non-trainable params: 4,096\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    BatchNormalization(input_shape=vgg.output_shape[1:])\n",
    "    , Dropout(.5)\n",
    "    , Dense(512, activation='relu')\n",
    "    , BatchNormalization()\n",
    "    , Dropout(.5)\n",
    "    , Dense(1024, activation='relu')\n",
    "    , BatchNormalization()\n",
    "    , Dropout(.5)\n",
    "    , Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 7s - loss: 0.2579 - acc: 0.9115 - val_loss: 0.8421 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.6471 - acc: 0.6567 - val_loss: 0.1451 - val_acc: 0.9430\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.6022 - acc: 0.6773 - val_loss: 0.1652 - val_acc: 0.9330\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5826 - acc: 0.6921 - val_loss: 0.1485 - val_acc: 0.9490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b3806b748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "model.fit(X_train_aug_1, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "model.fit(X_train_aug_2, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "model.fit(X_train_aug_3, y_train, epochs=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1645 - acc: 0.9316 - val_loss: 0.6391 - val_acc: 0.5005\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5943 - acc: 0.6830 - val_loss: 0.1807 - val_acc: 0.9380\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5701 - acc: 0.6978 - val_loss: 0.1185 - val_acc: 0.9530\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5674 - acc: 0.6993 - val_loss: 0.1451 - val_acc: 0.9405\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1572 - acc: 0.9363 - val_loss: 0.6776 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5782 - acc: 0.6900 - val_loss: 0.1905 - val_acc: 0.9320\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5661 - acc: 0.7002 - val_loss: 0.1099 - val_acc: 0.9560\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5559 - acc: 0.7044 - val_loss: 0.1094 - val_acc: 0.9610\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = .0001\n",
    "for _ in range(2):\n",
    "    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_1, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_2, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_3, y_train, epochs=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1560 - acc: 0.9387 - val_loss: 0.6435 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5622 - acc: 0.7022 - val_loss: 0.1723 - val_acc: 0.9430\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5521 - acc: 0.7046 - val_loss: 0.1229 - val_acc: 0.9575\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5474 - acc: 0.7123 - val_loss: 0.1465 - val_acc: 0.9490\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1500 - acc: 0.9392 - val_loss: 0.7095 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5581 - acc: 0.7033 - val_loss: 0.2023 - val_acc: 0.9430\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5418 - acc: 0.7121 - val_loss: 0.1178 - val_acc: 0.9565\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5379 - acc: 0.7214 - val_loss: 0.1501 - val_acc: 0.9425\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1420 - acc: 0.9433 - val_loss: 0.7163 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5442 - acc: 0.7110 - val_loss: 0.1820 - val_acc: 0.9425\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5348 - acc: 0.7190 - val_loss: 0.1272 - val_acc: 0.9530\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5342 - acc: 0.7238 - val_loss: 0.1146 - val_acc: 0.9625\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = .00001\n",
    "for _ in range(3):\n",
    "    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_1, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_2, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_3, y_train, epochs=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1526 - acc: 0.9395 - val_loss: 0.6192 - val_acc: 0.5795\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 7s - loss: 0.5367 - acc: 0.7171 - val_loss: 0.1951 - val_acc: 0.9455\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5316 - acc: 0.7221 - val_loss: 0.1190 - val_acc: 0.9555\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5232 - acc: 0.7257 - val_loss: 0.1164 - val_acc: 0.9595\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1438 - acc: 0.9433 - val_loss: 0.6778 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5358 - acc: 0.7213 - val_loss: 0.1832 - val_acc: 0.9495\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5262 - acc: 0.7235 - val_loss: 0.1172 - val_acc: 0.9605\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5196 - acc: 0.7300 - val_loss: 0.1225 - val_acc: 0.9510\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1449 - acc: 0.9422 - val_loss: 0.6843 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5244 - acc: 0.7246 - val_loss: 0.2009 - val_acc: 0.9485\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5222 - acc: 0.7277 - val_loss: 0.1273 - val_acc: 0.9485\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5128 - acc: 0.7343 - val_loss: 0.1189 - val_acc: 0.9580\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1424 - acc: 0.9441 - val_loss: 0.6773 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5276 - acc: 0.7265 - val_loss: 0.1917 - val_acc: 0.9515\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5175 - acc: 0.7333 - val_loss: 0.1237 - val_acc: 0.9555\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5147 - acc: 0.7327 - val_loss: 0.1218 - val_acc: 0.9580\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.1419 - acc: 0.9446 - val_loss: 0.6608 - val_acc: 0.5000\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5302 - acc: 0.7223 - val_loss: 0.2032 - val_acc: 0.9510\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5136 - acc: 0.7323 - val_loss: 0.1208 - val_acc: 0.9545\n",
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/1\n",
      "23000/23000 [==============================] - 6s - loss: 0.5114 - acc: 0.7329 - val_loss: 0.1125 - val_acc: 0.9610\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = .000001\n",
    "for _ in range(5):\n",
    "    model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_1, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_2, y_train, epochs=1, validation_data=(X_valid, y_valid))\n",
    "    model.fit(X_train_aug_3, y_train, epochs=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def get_learner():\n",
    "    model = Sequential([\n",
    "        BatchNormalization(input_shape=vgg.output_shape[1:])\n",
    "        , Dropout(.5)\n",
    "        , Dense(512, activation='relu')\n",
    "        , BatchNormalization()\n",
    "        , Dropout(.5)\n",
    "        , Dense(1024, activation='relu')\n",
    "        , BatchNormalization()\n",
    "        , Dropout(.5)\n",
    "        , Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_data():\n",
    "    X_train = load_array(path + 'train_vgg_average_pool_features_normalized.bc')\n",
    "    X_valid = load_array(path + 'valid_vgg_average_pool_features_normalized.bc')\n",
    "\n",
    "#     idxs = [i for i in range(len(X_train))]\n",
    "#     np.random.shuffle(idxs)\n",
    "#     idxs = idxs[:len(X_train)//2]\n",
    "        \n",
    "    y_train = keras.utils.np_utils.to_categorical(batches.classes)\n",
    "    y_valid = keras.utils.np_utils.to_categorical(val_batches.classes)\n",
    "    \n",
    "#     X_train = [X_train[i] for i in idxs]\n",
    "#     y_train = [y_train[i] for i in idxs]\n",
    "    \n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "    \n",
    "def train_learner(model, X_train, y_train, X_valid, y_valid, num_epochs=4, learning_rate=.001):\n",
    "    print(\"Training model for {} epochs with learning rate {}\".format(num_epochs, learning_rate))\n",
    "    \n",
    "    model.optimizer.lr = learning_rate\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_valid, y_valid), verbose=0)\n",
    "    \n",
    "def train_one_learner():\n",
    "    model = get_learner()\n",
    "    X_train, y_train, X_valid, y_valid = get_data()\n",
    "    \n",
    "    train_learner(model, X_train, y_train, X_valid, y_valid, num_epochs=4, learning_rate=.001)\n",
    "    train_learner(model, X_train, y_train, X_valid, y_valid, num_epochs=8, learning_rate=.0001)\n",
    "    train_learner(model, X_train, y_train, X_valid, y_valid, num_epochs=12, learning_rate=.00001)\n",
    "    #train_learner(model, X_train, y_train, X_valid, y_valid, num_epochs=16, learning_rate=.000001)\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jared/courses/deeplearning2/Kaggle/dogscats/data/'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_learner()\n",
    "X_train, y_train, X_valid, y_valid = get_data()\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 4 epochs with learning rate 0.001\n",
      "Training model for 8 epochs with learning rate 0.0001\n",
      "Training model for 12 epochs with learning rate 1e-05\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-04ca86178fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'ensamble_{}_2.h5'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mensamble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2551\u001b[0m         \"\"\"\n\u001b[1;32m   2552\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     'optimizer_config': {\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                         \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     },\n\u001b[1;32m    136\u001b[0m                     \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         config = {'lr': float(K.get_value(self.lr)),\n\u001b[0m\u001b[1;32m    449\u001b[0m                   \u001b[0;34m'beta_1'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                   \u001b[0;34m'beta_2'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mNumpy\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \"\"\"\n\u001b[0;32m-> 2130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "ensamble = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = train_one_learner()\n",
    "    model.save(path + 'ensamble_{}_2.h5'.format(i))\n",
    "    \n",
    "    ensamble.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ens_pred(ensamble ,arr):\n",
    "    ens_pred = []\n",
    "    for model in ensamble:\n",
    "        preds = model.predict(arr, batch_size=batch_size)\n",
    "        ens_pred.append(preds)\n",
    "    return np.stack(ens_pred).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)),\n",
    "                  K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "val_avg_preds2 = get_ens_pred(ensamble, X_valid)\n",
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(categorical_accuracy(y_valid, val_avg_preds2).eval().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
